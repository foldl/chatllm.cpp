# ChatLLM.cpp

[English](README.md)

[![License: MIT](https://img.shields.io/badge/license-MIT-blue)](LICENSE)

![](./docs/demo.gif)

åœ¨è®¡ç®—æœºï¼ˆCPUï¼‰ä¸Šå®æ—¶èŠå¤©ï¼Œå¯ [æ£€ç´¢å¢å¼ºç”Ÿæˆ](./docs/rag.md) ã€‚æ”¯æŒä»ä¸åˆ° 3B åˆ°è¶…è¿‡ 45B çš„ä¸€ç³»åˆ—æ¨¡å‹çš„æ¨ç†ã€‚åŸºäº [@ggerganov](https://github.com/ggerganov) çš„ [ggml](https://github.com/ggerganov/ggml)ï¼Œçº¯ C++ å®ç°ã€‚

| [æ”¯æŒçš„æ¨¡å‹](./docs/models.md) | [ä¸‹è½½é‡åŒ–æ¨¡å‹](https://modelscope.cn/models/judd2024/chatllm_quantized_models) |

## ç‰¹ç‚¹

- [x] å†…å­˜é«˜æ•ˆã€åŠ é€Ÿ CPU æ¨ç†ï¼šä½¿ç”¨ int4/int8 é‡åŒ–ã€ä¼˜åŒ–çš„ KV ç¼“å­˜å’Œå¹¶è¡Œè®¡ç®—ã€‚
- [x] é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼šå…³æ³¨åŸºäº Transformer çš„æ¨¡å‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
- [x] æµå¼ç”Ÿæˆï¼šæ‰“å­—æœºæ•ˆæœã€‚
- [x] è¿ç»­èŠå¤©ï¼šå†…å®¹é•¿åº¦å‡ ä¹æ— é™ã€‚

    æœ‰ä¸¤ç§æ–¹æ³•å¯ç”¨ï¼š_restart_ å’Œ _Shift_ã€‚è¯·å‚é˜… `--extending` é€‰é¡¹ã€‚

- [x] æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ ğŸ”¥

- [x] LoRAï¼šæœªå®Œæˆã€‚
- [x] Python/JavaScript/C [ç»‘å®š](./docs/binding.md)ï¼Œç½‘é¡µæ¼”ç¤ºï¼Œä»¥åŠæ›´å¤šå¯èƒ½æ€§ã€‚

## ä½¿ç”¨æ–¹æ³•

#### å‡†å¤‡å·¥ä½œ

å°† ChatLLM.cpp å­˜å‚¨åº“å…‹éš†åˆ°æœ¬åœ°è®¡ç®—æœºï¼š

```sh
git clone --recursive https://github.com/foldl/chatllm.cpp.git && cd chatllm.cpp
```

å¦‚æœåœ¨å…‹éš†å­˜å‚¨åº“æ—¶å¿˜è®°äº† `--recursive` æ ‡å¿—ï¼Œè¯·åœ¨ chatllm.cpp æ–‡ä»¶å¤¹ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```sh
git submodule update --init --recursive
```

#### é‡åŒ–æ¨¡å‹

**å¯ä»¥ä»[è¿™é‡Œ](https://modelscope.cn/models/judd2024/chatllm_quantized_models)ä¸‹è½½ä¸€äº›é‡åŒ–æ¨¡å‹ã€‚**

ä¸º `convert.py` å®‰è£…ä¾èµ–:

```sh
pip install -r requirements.txt
```

ä½¿ç”¨ `convert.py` å°†æ¨¡å‹è½¬æ¢ä¸ºé‡åŒ–çš„ GGML æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œè¦å°†æŸä¸ªæ¨¡å‹è½¬æ¢ä¸º q8_0ï¼ˆint8 é‡åŒ–ï¼‰GGML æ¨¡å‹ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```sh
# DeepSeek LLM Chat æ¨¡å‹
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a DeepSeek

# DeepSeek Coder æ¨¡å‹
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a DeepSeekCoder

# CodeLlaMA æ¨¡å‹
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a CodeLlaMA

# Yi models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a Yi

# WizardCoder models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a WizardCoder

# WizardLM models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a WizardLM

# WizardMath models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a WizardMath

# OpenChat models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a OpenChat

# Starling models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a Starling

# TigerBot models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a TigerBot

# Dolphin (based on Phi-2) models
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a DolphinPhi2

# NeuralBeagle14
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a NeuralBeagle

# CodeFuse-DeepSeek
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a CodeFuseDeepSeek

# CharacterGLM
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin -a CharacterGLM

# For other models, such as ChatLLM-6B, ChatLLM2-6B, InternLM, LlaMA, LlaMA-2, Baichuan-2, etc
python3 convert.py -i path/to/model -t q8_0 -o quantized.bin
```

è¯´æ˜ï¼šå¤§ä½“ä¸Šï¼Œä»…æ”¯æŒ HF æ ¼å¼ï¼›ç”Ÿæˆçš„ `.bin` æ–‡ä»¶æ ¼å¼ä¸åŒäºå½“å‰ `llama.cpp` é¡¹ç›®æ‰€ä½¿ç”¨çš„ GGUFã€‚

### æ„å»ºä¸è¿è¡Œ

ä½¿ç”¨ CMake ç¼–è¯‘é¡¹ç›®ï¼š

```sh
cmake -B build
# åœ¨ Linux æˆ– WSL ä¸Šï¼š
cmake --build build -j
# åœ¨ Windows ä¸Šä½¿ç”¨ MSVCï¼š
cmake --build build -j --config Release
```

ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸é‡åŒ–æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼š

```sh
./build/bin/main -m chatglm-ggml.bin                            # ChatGLM-6B
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
./build/bin/main -m llama2.bin  --seed 100                      # Llama-2-Chat-7B
# Hello! I'm here to help you with any questions or concerns ....
```

è¦åœ¨äº¤äº’æ¨¡å¼ä¸‹è¿è¡Œæ¨¡å‹ï¼Œè¯·æ·»åŠ  `-i` æ ‡å¿—ã€‚ä¾‹å¦‚ï¼š

```sh
# åœ¨ Windows ä¸Šï¼š
.\build\bin\Release\main -m model.bin -i

# åœ¨ Linuxï¼ˆæˆ– WSLï¼‰ä¸Šï¼š
rlwrap ./build/bin/main -m model.bin -i
```

åœ¨äº¤äº’æ¨¡å¼ä¸‹ï¼Œæ‚¨çš„èŠå¤©å†å²å°†ä½œä¸ºä¸‹ä¸€è½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚

è¿è¡Œ `./build/bin/main -h` ä»¥æ¢ç´¢æ›´å¤šé€‰é¡¹ï¼

## è‡´è°¢

* æœ¬é¡¹ç›®èµ·åˆæ˜¯å¯¹ [ChatGLM.cpp](https://github.com/li-plus/chatglm.cpp) çš„é‡æ„ï¼Œæ²¡æœ‰å®ƒï¼Œè¿™ä¸ªé¡¹ç›®å°†æ— æ³•å®ç°ã€‚

* æ„Ÿè°¢é‚£äº›å‘å¸ƒäº†æ¨¡å‹æºä»£ç å’Œæ£€æŸ¥ç‚¹çš„äººã€‚

## æ³¨æ„

è¿™ä¸ªé¡¹ç›®æ˜¯æˆ‘ç”¨æ¥å­¦ä¹ æ·±åº¦å­¦ä¹ å’Œ GGML çš„ä¸šä½™é¡¹ç›®ï¼Œç›®å‰æ­£åœ¨ç§¯æå¼€å‘ä¸­ã€‚æ¬¢è¿ä¿®å¤ bug çš„ PRï¼Œä½†ä¸æ¥å—åŠŸèƒ½æ€§çš„ PRã€‚
